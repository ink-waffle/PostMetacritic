{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:16:13.067819Z",
     "end_time": "2023-12-28T02:16:31.335334Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import openai\n",
    "import json\n",
    "from Keys import openai_keys\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keybert import KeyBERT\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "openai.organization = openai_keys['organization']\n",
    "openai.api_key = openai_keys['api_key']\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input=[text], model=model)['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the JSON data from the file\n",
    "with open('Source/embeddings-timur.json', 'r') as json_file:\n",
    "    embeddings = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:16:31.337333Z",
     "end_time": "2023-12-28T02:16:32.982224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "clf = joblib.load('models/embeddingsForest_4.pkl')\n",
    "NNmodel = load_model('models/embeddingsNN.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:16:33.726921Z",
     "end_time": "2023-12-28T02:16:37.250172Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self, title, text, embedding, sentiment=1, probas=[0, 0, 0]):\n",
    "        self.title = title\n",
    "        self.raw = text\n",
    "        self.embedding = embedding\n",
    "        self.sentiment = sentiment\n",
    "        self.cluster = None\n",
    "        self.probs = probas\n",
    "        self.tsne = None\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, clusterIndex, element):\n",
    "        self.clusterIndex = clusterIndex\n",
    "        self.elements = [element]\n",
    "        self.clusterName = \"\"\n",
    "        self.keywords = Counter()\n",
    "        self.clusterSize = (0, 0)\n",
    "        self.clusterBuffer = element.embedding\n",
    "        self.sentimentDistribution = np.array([1, 0, 0])\n",
    "        self.sentimentDistribution[element.sentiment] += 1\n",
    "        self.subClusters = dict()\n",
    "\n",
    "    def calculateClusterSize(self):\n",
    "        tmp = np.sum(np.square(self.clusterBuffer - np.mean(self.clusterBuffer, axis=0, keepdims=True)), axis=1)\n",
    "        self.clusterSize = (np.mean(tmp).item(), np.max(tmp).item())\n",
    "        return self.clusterSize\n",
    "\n",
    "    def addPoint(self, point):\n",
    "        self.elements.append(point)\n",
    "        self.clusterBuffer = np.vstack((self.clusterBuffer, point.embedding))\n",
    "        self.sentimentDistribution[point.sentiment] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:16:38.408874Z",
     "end_time": "2023-12-28T02:16:38.435876Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "sentiments = list()\n",
    "iter = 0\n",
    "X = list()\n",
    "for analysis in embeddings:\n",
    "    emb = np.array(analysis[\"embedding\"])\n",
    "    # y_pr = clf.predict_proba(emb)\n",
    "    # y = clf.predict(emb).item()\n",
    "    X.append(emb)\n",
    "    sentiments.append(Analysis(analysis[\"title\"], analysis[\"text\"], emb))\n",
    "X = np.array(X)\n",
    "y_pr = NNmodel.predict(X)\n",
    "y = np.argmax(y_pr, axis=1, keepdims=True)\n",
    "for i in range(y.shape[0]):\n",
    "    sentiments[i].sentiment = y[i].item()\n",
    "    sentiments[i].probs = y_pr[i]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:16:44.488929Z",
     "end_time": "2023-12-28T02:16:45.656120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:110: UserWarning:\n",
      "\n",
      "Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n",
      "\n",
      "KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=6.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering = DBSCAN(eps=0.46, min_samples=3).fit(X)\n",
    "X_left = list()\n",
    "mapping = dict()\n",
    "clusters = dict()\n",
    "for i in range(len(sentiments)):\n",
    "    if clustering.labels_[i] == -1:\n",
    "        X_left.append(sentiments[i].embedding)\n",
    "        mapping[len(X_left) - 1] = i\n",
    "    else:\n",
    "        if clustering.labels_[i] in clusters:\n",
    "            clusters[clustering.labels_[i]].addPoint(sentiments[i])\n",
    "        else:\n",
    "            clusters[clustering.labels_[i]] = Cluster(clustering.labels_[i], sentiments[i])\n",
    "        sentiments[i].cluster = clusters[clustering.labels_[i]]\n",
    "\n",
    "# initialClusters = clusters.copy()\n",
    "clusters_n = len(set(clustering.labels_)) - 1\n",
    "# bestClustering = None\n",
    "# bestClusteringSize = np.inf\n",
    "# for trial in range(10):\n",
    "# clusters = initialClusters.copy()\n",
    "clustering_left = KMeans(n_clusters=24, init='k-means++', tol=1e-7, max_iter=1000).fit(\n",
    "    np.array(X_left).reshape((-1, len(sentiments[0].embedding))))\n",
    "for i in range(len(X_left)):\n",
    "    if clustering_left.labels_[i] + clusters_n in clusters:\n",
    "        clusters[clustering_left.labels_[i] + clusters_n].addPoint(sentiments[mapping[i]])\n",
    "    else:\n",
    "        clusters[clustering_left.labels_[i] + clusters_n] = Cluster(clustering_left.labels_[i] + clusters_n,\n",
    "                                                                    sentiments[mapping[i]])\n",
    "    sentiments[mapping[i]].cluster = clusters[clustering_left.labels_[i] + clusters_n]\n",
    "\n",
    "for clust in clusters.values():\n",
    "    clust.calculateClusterSize()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:17:19.684391Z",
     "end_time": "2023-12-28T02:17:22.995829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "sentimentDictionary = {'positive': list(), 'negative': list(), 'neutral': list()}\n",
    "for sent in sentiments:\n",
    "    if sent.sentiment == 2 or (sent.probs[0] - sent.probs[2] <= 0.05):\n",
    "        sentimentDictionary['positive'].append(sent)\n",
    "    elif sent.sentiment == 1 or (\n",
    "            -0.05 <= sent.probs[0] - sent.probs[1] <= 0.05 and -0.05 <= sent.probs[2] - sent.probs[1] <= 0.05):\n",
    "        sentimentDictionary['neutral'].append(sent)\n",
    "    elif sent.sentiment == 0:\n",
    "        sentimentDictionary['negative'].append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T03:26:16.008646Z",
     "end_time": "2023-12-28T03:26:16.040422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "sentimentClusters = {'positive': list(), 'negative': list(), 'neutral': list()}\n",
    "for clust in clusters.values():\n",
    "    if clust.sentimentDistribution[0] > clust.sentimentDistribution[1] and clust.sentimentDistribution[0] > \\\n",
    "            clust.sentimentDistribution[2]:\n",
    "        sentimentClusters['negative'].append(clust)\n",
    "    elif clust.sentimentDistribution[1] > clust.sentimentDistribution[0] and clust.sentimentDistribution[1] > \\\n",
    "            clust.sentimentDistribution[2]:\n",
    "        sentimentClusters['neutral'].append(clust)\n",
    "    else:\n",
    "        sentimentClusters['positive'].append(clust)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T03:25:02.980471Z",
     "end_time": "2023-12-28T03:25:03.012637Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=69)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "for i in range(len(sentiments)):\n",
    "    sentiments[i].tsne = X_tsne[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T02:19:24.616228Z",
     "end_time": "2023-12-28T02:20:05.296213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'Product/Timur/Clusters.html'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insert_line_breaks(text, max_length=75):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in words:\n",
    "        if len(current_line + word) <= max_length:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "    lines.append(current_line)\n",
    "    return '<br>'.join(lines).strip()\n",
    "\n",
    "def cap_str(text, cap_len=75):\n",
    "    if len(text) < cap_len:\n",
    "        return text\n",
    "    else:\n",
    "        return text[:cap_len] + '...'\n",
    "\n",
    "Hover_Info = [0] * len(sentiments)\n",
    "for i in range(len(sentiments)):\n",
    "    Hover_Info[i] = \"[\" + sentiments[i].title + \"]<br>\" + insert_line_breaks(sentiments[i].raw)\n",
    "    # Hover_Info[i] = sentiments[i].raw\n",
    "\n",
    "sent_packed = [0] * len(sentiments)\n",
    "for i in range(len(sentiments)):\n",
    "    sent_packed[i] = sentiments[i].sentiment\n",
    "\n",
    "clust_packed = [0] * len(sentiments)\n",
    "for i in range(len(sentiments)):\n",
    "    clust_packed[i] = sentiments[i].cluster.clusterIndex\n",
    "\n",
    "df = pd.DataFrame(np.hstack((X_tsne, np.array(sent_packed).reshape(-1, 1), np.array(clust_packed).reshape(-1, 1))),\n",
    "                  columns=['x', 'y', 'sentiment', 'cluster'])\n",
    "df['sentiment'] = df['sentiment'].astype('str')\n",
    "df['cluster'] = df['cluster'].astype('str')\n",
    "\n",
    "capSize = list(reversed(sorted([len(clust.elements) for clust in clusters.values()])))[25]\n",
    "annotations = []\n",
    "for clust in clusters.values():\n",
    "    if len(clust.elements) < capSize:\n",
    "        continue\n",
    "\n",
    "    pos = np.mean(np.array([elem.tsne for elem in clust.elements]), axis=0, keepdims=True)\n",
    "    annotations.append(go.layout.Annotation(x=pos[0, 0], y=pos[0,1], text=insert_line_breaks(cap_str(clust.clusterName, cap_len=160)), showarrow=False, ax=0, ay=0, bgcolor='rgba(0, 0, 0, 0.5)', borderpad=0,\n",
    "                                            font=dict(\n",
    "                                            family=\"Arial, sans-serif\",  # You can choose your font family here\n",
    "                                            size=11,  # Set the font size here\n",
    "                                            color='white'\n",
    "                                        )))\n",
    "\n",
    "fig = px.scatter(df, x='x', y='y', color='sentiment', hover_name=Hover_Info,\n",
    "                 color_discrete_sequence=[\"purple\", \"gray\", \"orange\"])\n",
    "fig.update_traces(marker={'size': 9})\n",
    "fig.update_layout(\n",
    "    # title=\"t-SNE of Sentiments\",\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"\",\n",
    "    template='plotly_white',\n",
    "    hoverlabel=dict(font_size=15),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    annotations=annotations\n",
    ")\n",
    "fig.update_traces(marker=dict(showscale=False))\n",
    "fig.update_layout(showlegend=False)\n",
    "pyo.plot(fig, filename='Product/Timur/Sentiments.html', auto_open=False)\n",
    "\n",
    "# df = pd.DataFrame(X_tsne, columns=['x', 'y'])\n",
    "colors_ = cm.nipy_spectral(np.linspace(0, 1, len(clusters)))\n",
    "# Convert the colors from RGBA to a format accepted by Plotly\n",
    "colors = ['rgb' + str(tuple(int(c * 255) for c in color[:-1])) for color in colors_]\n",
    "fig = px.scatter(df, x='x', y='y', color='cluster', color_discrete_sequence=colors, hover_name=Hover_Info, )\n",
    "fig.update_traces(marker={'size': 9})\n",
    "fig.update_layout(\n",
    "    # title=\"t-SNE of Clusters\",\n",
    "    xaxis_title=\"\",\n",
    "    yaxis_title=\"\",\n",
    "    template='plotly_white',\n",
    "    hoverlabel=dict(font_size=15),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    annotations=annotations\n",
    ")\n",
    "fig.update_traces(marker=dict(showscale=False))\n",
    "fig.update_layout(showlegend=False)\n",
    "pyo.plot(fig, filename='Product/Timur/Clusters.html', auto_open=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T03:23:58.089638Z",
     "end_time": "2023-12-28T03:23:59.436099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning:\n",
      "\n",
      "Recommended: pip install sacremoses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_model = T5ForConditionalGeneration.from_pretrained('t5-small').to('cuda')\n",
    "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "translation_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ru-en').to('cuda')\n",
    "translation_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\n",
    "\n",
    "\n",
    "for clust in clusters.values():\n",
    "    # start_time = time.time()\n",
    "    inp = []\n",
    "    for elem in clust.elements:\n",
    "        if (len(elem.raw) > 300 and len(clust.elements) > 6) or len(elem.raw) > 500:\n",
    "            # inp.append(clust.elements[i].raw[:300])\n",
    "            continue\n",
    "        inp.append(elem.raw)\n",
    "        if len(inp) >= 10:\n",
    "            break\n",
    "    inputs = translation_tokenizer('\\n'.join(inp), return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        translated = translation_model.generate(**inputs).cpu()\n",
    "\n",
    "    translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    # input_ids = summary_tokenizer(\"summarize: \" + '\\n'.join(inp), return_tensors=\"pt\", truncation=True).input_ids.to('cuda')\n",
    "    input_ids = summary_tokenizer(\"summarize: \" + translated_text, return_tensors=\"pt\", truncation=True).input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = summary_model.generate(\n",
    "            input_ids,\n",
    "            max_length=30,  # Maximum length of the generated summary\n",
    "            min_length=15,  # Minimum length of the generated summary\n",
    "            length_penalty=0.25,  # Higher values encourage longer summaries\n",
    "            num_beams=2,  # Number of beams for beam search\n",
    "            early_stopping=False,  # Stop when a good candidate is found even if not all beams are finished\n",
    "            repetition_penalty=1,\n",
    "            # temperature=0.9\n",
    "        ).cpu()\n",
    "    summary = summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True, max_words_length=35)\n",
    "    clust.clusterName = summary\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # time_taken = end_time - start_time\n",
    "    # print(f'Time taken: {time_taken} seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-28T03:02:28.852790Z",
     "end_time": "2023-12-28T03:05:12.105088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-ru-en'\n",
    "translation_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\n",
    "translation_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ru-en')\n",
    "\n",
    "# Your Russian text\n",
    "text = 'I couldnt figure it out for a long time, and in eight minutes, things got back to normal.'\n",
    "\n",
    "# Tokenize the text and translate\n",
    "inputs = translation_tokenizer(text, return_tensors=\"pt\", truncation=True).to('cuda')\n",
    "translated = translation_model.generate(**inputs)\n",
    "\n",
    "# Decode the translated text\n",
    "translated_text = translation_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-21T13:45:35.128660Z",
     "end_time": "2023-10-21T13:45:40.847524Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "json_clusters = []\n",
    "for clust in clusters.values():\n",
    "    json_elements = []\n",
    "    for elem in clust.elements:\n",
    "        json_elements.append({\"text\": elem.raw, \"sentiment\": elem.sentiment, \"position\": elem.tsne.tolist()})\n",
    "    json_clusters.append({\"cluster_name\": clust.clusterName, \"cluster_sentiment\": clust.sentimentDistribution.tolist(), \"elements\": json_elements})\n",
    "\n",
    "with open('Product/Temp/markus.json', 'w') as json_file:\n",
    "    json.dump(json_clusters, json_file, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-21T17:49:18.880704Z",
     "end_time": "2023-10-21T17:49:18.941090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-21T16:12:45.158061Z",
     "end_time": "2023-10-21T16:12:45.194200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
